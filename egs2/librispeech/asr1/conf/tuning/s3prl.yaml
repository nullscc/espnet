# Trained with Ampere A6000(48GB) x 2 GPUs. It takes about 10 days.
# batch_type: numel
# batch_bins: 8000000
batch_type: unsorted
batch_size: 8
accum_grad: 3
max_epoch: 100
patience: none
init: none
best_model_criterion:
-   - valid
    - acc
    - max
keep_nbest_models: 10
unused_parameters: true
freeze_param: [
"encoder.upstream"
]

frontend: none

input_size: 2048
encoder: s3prl
encoder_conf:
    frontend_conf:
        upstream: wavlm_local    # Note: If the upstream is changed, please change the input_size in the preencoder.
        upstream_ckpt: /ssdhome/lgx521/pkg/pre-training/pre_trained_model/wavlm/WavLM-Large.pt
    download_dir: ./hub
    multilayer_feature: True

model_conf:
    ctc_weight: 1.0
    lsm_weight: 0.1
    length_normalized_loss: false
    extract_feats_in_collect_stats: false   # Note: "False" means during collect stats (stage 10), generating dummy stats files rather than extract_feats by forward frontend.

optim: adam
optim_conf:
    lr: 0.0025
scheduler: warmuplr
scheduler_conf:
    warmup_steps: 40000

specaug: none

